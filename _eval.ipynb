{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC 2007:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientDet D0 (100 epochs/lr=1e-3 finetuning):\n",
    "\n",
    "| **Loss**          | **IoU50**  | **IoU75**  | **IoU50:95** | **PIoU50** | **PIoU75** | **PIoU50:95** |\n",
    "| ----------------  | ---------- | ---------- | ------------ | ---------- | ---------- | ------------- |\n",
    "| PIoU 10xL3->2xL1¹ | **0.7261** | 0.4424     | 0.4260       | **0.7670** | **0.6415** | **0.5676**    |\n",
    "| GIoU              | 0.7045     | 0.4396     | 0.4223       | 0.7426     | 0.6212     | 0.5533        |\n",
    "| DIoU              | *0.7007*   | **0.4474** | 0.4264       | 0.7352     | 0.6226     | 0.5531        |\n",
    "| CIoU              | 0.7053     | 0.4535     | **0.4294**   | 0.7442     | 0.6304     | 0.5587        |\n",
    "| Smooth L1         | 0.7020     | *0.4202*   | *0.4072*     | *0.7409*   | *0.6126*   | *0.5449*      |\n",
    "\n",
    "¹freezed *a* and *b* for the first 1000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### EfficientDet D0 (100 epochs finetuning):\n",
    "\n",
    "| **Loss**         | **IoU50** | **IoU75** | **IoU50:95** | **PIoU50** | **PIoU75** | **PIoU50:95** |\n",
    "| ---------------- | --------- | --------- | ------------ | ---------- | ---------- | ------------- |\n",
    "| *lr=1e-4*        |           |           |              |            |            |               |\n",
    "| Smooth L1        | 0.7207    | 0.3132    | 0.3479       | 0.7378     | 0.5816     | 0.5072        |\n",
    "| PIoU L1          | 0.7186    | 0.3196    | 0.3508       | 0.7393     | 0.5768     | 0.5074        |\n",
    "| PIoU L2          | 0.7043    | 0.3090    | 0.3443       | 0.7305     | 0.5722     | 0.5003        |\n",
    "| PIoU L3          | 0.6992    | 0.3033    | 0.3388       | 0.7271     | 0.5679     | 0.4959        |\n",
    "| PIoU L3+L1       | 0.6665    | 0.2905    | 0.3319       | 0.7210     | 0.5606     | 0.4900        |\n",
    "| GIoU             | 0.7257    | 0.3481    | 0.3672       | 0.7405     | 0.5874     | 0.5169        |\n",
    "| *lr=1e-3*        |           |           |              |            |            |               |\n",
    "| PIoU L3->L1      | 0.6986    | 0.4109    | 0.3999       | 0.7423     | 0.6075     | 0.5399        |\n",
    "| PIoU L1          | 0.6651    | 0.3711    | 0.3714       | 0.7095     | 0.5811     | 0.5101        |\n",
    "| 2x PIoU L1       | 0.6833    | 0.3940    | 0.3927       | 0.7242     | 0.5988     | 0.5273        |\n",
    "| PIoU L3¹->2xL1   | 0.7009    | 0.4141    | 0.4043       | 0.7428     | 0.6160     | 0.5442        |\n",
    "| PIoU 20xL3²>2xL1 | 0.7209    | 0.4246    | 0.4159       | 0.7634     | 0.6379     | 0.5606        |\n",
    "| PIoU L3²->20xL2  | 0.7222    | 0.4341    | 0.4209       | 0.7596     | 0.6368     | 0.5624        |\n",
    "| PIoU L3->4xL1    | 0.7224    | 0.4393    | 0.4240       | 0.7606     | 0.6358     | 0.5637        |\n",
    "| 2XGIoU *(use?)*  | 0.7066    | 0.4611    | 0.4358       | 0.7418     | 0.6245     | 0.5602        |\n",
    "| *250 epochs*     |           |           |              |            |            |               |\n",
    "| PIoU L3²->2xL1   | 0.7248    | 0.4393    | 0.4250       | 0.7648     | 0.6457     | 0.5658        |\n",
    "| PIoU L3->Smooth  | 0.7192    | 0.4267    | 0.4150       | 0.7626     | 0.6370     | 0.5612        |\n",
    "| 2xGIoU           | 0.7020    | 0.4678    | 0.4362       | 0.7385     | 0.6204     | 0.5589        |\n",
    "\n",
    "¹ started training with L3\n",
    "\n",
    "² started training with L3 (after freezed a&b in the first epochs)\n",
    "\n",
    "#### EfficientDet D1 (150 epochs finetuning):\n",
    "\n",
    "| **Loss**     | **IoU50** | **IoU75** | **IoU50:95** | **PIoU50** | **PIoU75** | **PIoU50:95** |\n",
    "| -----------  | --------- | --------- | ------------ | ---------- | ---------- | ------------- |\n",
    "| Smooth L1    | 0.7364    | 0.4640    | 0.4410       | 0.7742     | 0.6549     | 0.5781        |\n",
    "| GIoU         | 0.7237    | 0.4866    | 0.4534       | 0.7583     | 0.6422     | 0.5772        |\n",
    "| PIoU L3->L1¹ | 0.7071    | 0.4419    | 0.4241       | 0.7438     | 0.6320     | 0.5590        |\n",
    "\n",
    "¹trained 59 epoch and reached nan values\n",
    "\n",
    "### VOC 2007 + 2012:\n",
    "\n",
    "| **Loss**   | **IoU50** | **IoU75** | **IoU50:95** | **PIoU50** | **PIoU75** | **PIoU50:95** |\n",
    "| ---------- | --------- | --------- | ------------ | ---------- | ---------- | ------------- |\n",
    "| Smooth L1  | XXXXXX    | XXXXXX    | XXXXXX       | XXXXXX     | XXXXXX     | XXXXXX        |\n",
    "| PIoU L1    | XXXXXX    | XXXXXX    | XXXXXX       | XXXXXX     | XXXXXX     | XXXXXX        |\n",
    "| GIoU       | XXXXXX    | XXXXXX    | XXXXXX       | XXXXXX     | XXXXXX     | XXXXXX        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.common import evaluate, evaluate_mAP\n",
    "from generators.pascal import PascalVocGenerator\n",
    "from model import efficientdet\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 0\n",
    "weighted_bifpn = False\n",
    "common_args = {\n",
    "    'batch_size': 1,\n",
    "    'phi': phi,\n",
    "}\n",
    "test_generator = PascalVocGenerator(\n",
    "    '/datasets/dataset/VOCdevkit/VOC2007',\n",
    "    'test',\n",
    "    shuffle_groups=False,\n",
    "    skip_truncated=False,\n",
    "    skip_difficult=True,\n",
    "    **common_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "model_path = 'checkpoints/voc_ciou_finetuned/pascal.h5'\n",
    "input_shape = (test_generator.image_size, test_generator.image_size)\n",
    "anchors = test_generator.anchors\n",
    "num_classes = test_generator.num_classes()\n",
    "model, prediction_model = efficientdet(phi=phi, num_classes=num_classes, weighted_bifpn=weighted_bifpn)\n",
    "prediction_model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (4952 of 4952) |###| Elapsed Time: 0:02:19 Time:  0:02:19\n",
      "Parsing annotations: 100% (4952 of 4952) || Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_fp=99961.0, num_tp=10228.0\n",
      "285 instances of class aeroplane with average precision: 0.7664\n",
      "337 instances of class bicycle with average precision: 0.8283\n",
      "459 instances of class bird with average precision: 0.6798\n",
      "263 instances of class boat with average precision: 0.5679\n",
      "469 instances of class bottle with average precision: 0.5004\n",
      "213 instances of class bus with average precision: 0.7977\n",
      "1201 instances of class car with average precision: 0.8133\n",
      "358 instances of class cat with average precision: 0.8194\n",
      "756 instances of class chair with average precision: 0.4671\n",
      "244 instances of class cow with average precision: 0.7282\n",
      "206 instances of class diningtable with average precision: 0.6491\n",
      "489 instances of class dog with average precision: 0.7990\n",
      "348 instances of class horse with average precision: 0.8518\n",
      "325 instances of class motorbike with average precision: 0.7969\n",
      "4528 instances of class person with average precision: 0.7802\n",
      "480 instances of class pottedplant with average precision: 0.4245\n",
      "242 instances of class sheep with average precision: 0.6801\n",
      "239 instances of class sofa with average precision: 0.6616\n",
      "282 instances of class train with average precision: 0.8013\n",
      "308 instances of class tvmonitor with average precision: 0.6922\n",
      "mAP0.5:0.95 = 0.7053\n"
     ]
    }
   ],
   "source": [
    "average_precisions = evaluate(test_generator, prediction_model, visualize=False)\n",
    "\n",
    "# compute per class average precision\n",
    "total_instances = []\n",
    "precisions = []\n",
    "for label, (average_precision, num_annotations) in average_precisions.items():\n",
    "    print('{:.0f} instances of class'.format(num_annotations), test_generator.label_to_name(label),\n",
    "          'with average precision: {:.4f}'.format(average_precision))\n",
    "    total_instances.append(num_annotations)\n",
    "    precisions.append(average_precision)\n",
    "mean_ap = sum(precisions) / sum(x > 0 for x in total_instances)\n",
    "print('mAP0.5:0.95 = {:.4f}'.format(mean_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running network: 100% (4952 of 4952) |###| Elapsed Time: 0:02:14 Time:  0:02:14\n",
      "Parsing annotations: 100% (4952 of 4952) || Elapsed Time: 0:00:01 Time:  0:00:01\n",
      "Evaluating threshold 0.50: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.55:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.50 = 0.7053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.55: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.60:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.55 = 0.6813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.60: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.65:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.60 = 0.6459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.65: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.70:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.65 = 0.5979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.70: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.75:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.70 = 0.5336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.75: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.80:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.75 = 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.80: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.85:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.80 = 0.3497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.85: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.90:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.85 = 0.2221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.90: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "Evaluating threshold 0.95:   5% (1 of 20) || Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.90 = 0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating threshold 0.95: 100% (20 of 20) || Elapsed Time: 0:00:05 Time:  0:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP0.95 = 0.0108\n",
      "num_fp=1037917.0, num_tp=63973.0\n",
      "285 instances of class aeroplane with average precision: 0.4583\n",
      "337 instances of class bicycle with average precision: 0.5115\n",
      "459 instances of class bird with average precision: 0.3753\n",
      "263 instances of class boat with average precision: 0.2980\n",
      "469 instances of class bottle with average precision: 0.2417\n",
      "213 instances of class bus with average precision: 0.6103\n",
      "1201 instances of class car with average precision: 0.5511\n",
      "358 instances of class cat with average precision: 0.5525\n",
      "756 instances of class chair with average precision: 0.2359\n",
      "244 instances of class cow with average precision: 0.4385\n",
      "206 instances of class diningtable with average precision: 0.3781\n",
      "489 instances of class dog with average precision: 0.4963\n",
      "348 instances of class horse with average precision: 0.5527\n",
      "325 instances of class motorbike with average precision: 0.4670\n",
      "4528 instances of class person with average precision: 0.4194\n",
      "480 instances of class pottedplant with average precision: 0.1882\n",
      "242 instances of class sheep with average precision: 0.4227\n",
      "239 instances of class sofa with average precision: 0.4289\n",
      "282 instances of class train with average precision: 0.5304\n",
      "308 instances of class tvmonitor with average precision: 0.4308\n",
      "mAP0.5:0.95 = 0.4294\n"
     ]
    }
   ],
   "source": [
    "average_precisions = evaluate_mAP(test_generator, prediction_model, method='iou', visualize=False)\n",
    "\n",
    "# compute per class average precision\n",
    "total_instances = []\n",
    "precisions = []\n",
    "for label, (average_precision, num_annotations) in average_precisions.items():\n",
    "    print('{:.0f} instances of class'.format(num_annotations), test_generator.label_to_name(label),\n",
    "          'with average precision: {:.4f}'.format(average_precision))\n",
    "    total_instances.append(num_annotations)\n",
    "    precisions.append(average_precision)\n",
    "mean_ap = sum(precisions) / sum(x > 0 for x in total_instances)\n",
    "print('mAP0.5:0.95 = {:.4f}'.format(mean_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f6d9541a33cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maverage_precisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_mAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'piou'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# compute per class average precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprecisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workdir/msc-piou/EfficientDet-unofficial/eval/common.py\u001b[0m in \u001b[0;36mevaluate_mAP\u001b[0;34m(generator, model, method, start_threshold, score_threshold, max_detections, visualize, epoch)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_overlap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'piou'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_overlap_piou\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# gather all detections and annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workdir/msc-piou/EfficientDet-unofficial/utils/compute_overlap_piou.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# --------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "average_precisions = evaluate_mAP(test_generator, prediction_model, method='piou', visualize=False)\n",
    "\n",
    "# compute per class average precision\n",
    "total_instances = []\n",
    "precisions = []\n",
    "for label, (average_precision, num_annotations) in average_precisions.items():\n",
    "    print('{:.0f} instances of class'.format(num_annotations), test_generator.label_to_name(label),\n",
    "          'with average precision: {:.4f}'.format(average_precision))\n",
    "    total_instances.append(num_annotations)\n",
    "    precisions.append(average_precision)\n",
    "mean_ap = sum(precisions) / sum(x > 0 for x in total_instances)\n",
    "print('mAP0.5:0.95 = {:.4f}'.format(mean_ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(path_training, sufix='finetuned'):\n",
    "    # open training info\n",
    "    path_csv = glob(os.path.join(path_training+'*', '*.csv'), recursive=True)\n",
    "    dfs = {'finetuned' if sufix in path else 'freezed': pd.read_csv(path) for path in path_csv}\n",
    "\n",
    "    # freezed epochs\n",
    "    freeze_epochs = max(dfs['freezed']['epoch'])\n",
    "    dfs['finetuned']['epoch'] += freeze_epochs\n",
    "\n",
    "    # join infos\n",
    "    df = pd.concat([dfs['freezed'], dfs['finetuned']])\n",
    "    df['epoch'] += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(name, metric_name, df, diff=False):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.grid('on')\n",
    "    if diff:\n",
    "        from scipy.signal import savgol_filter\n",
    "        adj = lambda x: savgol_filter(x[50:], 51, 3)\n",
    "        epochs = df['epoch'][51:]\n",
    "        plt.plot(epochs, adj(np.diff(df[f'val_{metric_name}loss'])),\\\n",
    "                 epochs, adj(np.diff(df[f'{metric_name}loss'])), linestyle='dashed')\n",
    "        plt.legend(['val', 'train'])\n",
    "        \n",
    "    else:\n",
    "        plt.plot(df['epoch'], df[f'val_{metric_name}loss'], df['epoch'], df[f'{metric_name}loss'])\n",
    "        plt.axvline(x=50, color='r', linestyle='--')\n",
    "        plt.legend(['val', 'train'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(f'{metric_name}loss')\n",
    "    plt.title(name)\n",
    "    \n",
    "    plt.savefig(f'saves/{metric_name}{name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('checkpoints/voc_piou_initl3_20xl2_freezed_finetuned/pascal_history.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric('PIoU L3->20xL2', 'regression_', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l1 = get_df('checkpoints/voc_l1')\n",
    "\n",
    "df_giou = get_df('checkpoints/voc_giou')\n",
    "\n",
    "df_pioul1 = get_df('checkpoints/voc_piou_l1')\n",
    "df_pioul3 = get_df('checkpoints/voc_piou_l3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric('Smooth L1', 'classification_', df_l1)\n",
    "plot_metric('GIoU', 'classification_', df_giou)\n",
    "plot_metric('PIoU L1', 'classification_', df_pioul1)\n",
    "plot_metric('PIoU L3', 'classification_', df_pioul3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric('Smooth L1', 'regression_', df_l1)\n",
    "plot_metric('GIoU', 'regression_', df_giou)\n",
    "plot_metric('PIoU L1', 'regression_', df_pioul1)\n",
    "plot_metric('PIoU L3', 'regression_', df_pioul3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric('Smooth L1 diff', 'regression_', df_l1, diff=True)\n",
    "plot_metric('GIoU diff', 'regression_', df_giou, diff=True)\n",
    "plot_metric('PIoU L1 diff', 'regression_', df_pioul1, diff=True)\n",
    "plot_metric('PIoU L3 diff', 'regression_', df_pioul3, diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
